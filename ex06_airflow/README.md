# Exercice 06 â€“ Orchestration Airflow

## Objectif

L'exercice **EX06** a pour objectif de mettre en place l'**orchestration automatisÃ©e** de l'ensemble du pipeline Big Data Ã  l'aide d'**Apache Airflow**.

## Architecture cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AIRFLOW                                  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    DAG: nyc_taxi_pipeline                â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  EX01  â”‚â”€â”€â”€â–¶â”‚  EX02  â”‚â”€â”€â”€â–¶â”‚  EX03  â”‚â”€â”€â”€â–¶â”‚  EX04  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚Retrieveâ”‚    â”‚Ingest  â”‚    â”‚  DW    â”‚    â”‚  BI    â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                     â”‚                                    â”‚  â”‚
â”‚  â”‚                     â–¼                                    â”‚  â”‚
â”‚  â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚  â”‚
â”‚  â”‚               â”‚  EX05  â”‚                                 â”‚  â”‚
â”‚  â”‚               â”‚   ML   â”‚                                 â”‚  â”‚
â”‚  â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Composants Airflow

### Infrastructure

| Composant        | Description                              |
|------------------|------------------------------------------|
| Airflow Webserver| Interface web (port 8080)                |
| Airflow Scheduler| Planification des DAGs                   |
| Airflow Worker   | ExÃ©cution des tÃ¢ches                     |
| PostgreSQL       | Backend metadata Airflow                 |
| Redis            | Message broker (CeleryExecutor)          |

### DAG principal

Le DAG `nyc_taxi_pipeline` orchestre l'ensemble du pipeline :

```
ex01_data_retrieval
        â”‚
        â–¼
ex02_data_ingestion
        â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â–¼         â–¼
ex03_dw   ex05_ml
   â”‚
   â–¼
ex04_dashboard_refresh
```

## Structure du projet (Ã  implÃ©menter)

```
ex06_airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ nyc_taxi_pipeline.py      # DAG principal
â”‚   â”œâ”€â”€ ex01_dag.py               # DAG EX01
â”‚   â”œâ”€â”€ ex02_dag.py               # DAG EX02
â”‚   â””â”€â”€ common/
â”‚       â”œâ”€â”€ spark_submit.py       # Helper spark-submit
â”‚       â””â”€â”€ config.py             # Configuration
â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ operators/
â”‚       â””â”€â”€ spark_submit_operator.py
â”œâ”€â”€ docker-compose.airflow.yml    # Docker Compose Airflow
â”œâ”€â”€ .env                          # Variables d'environnement
â””â”€â”€ README.md
```

## Configuration

### Variables Airflow

| Variable              | Description                    |
|-----------------------|--------------------------------|
| `spark_master_url`    | URL Spark Master               |
| `minio_endpoint`      | URL MinIO                      |
| `postgres_conn_id`    | Connection ID PostgreSQL       |
| `default_year`        | AnnÃ©e par dÃ©faut               |
| `default_month`       | Mois par dÃ©faut                |

### Connections

| Connection ID   | Type       | Description           |
|-----------------|------------|-----------------------|
| `spark_default` | Spark      | Cluster Spark         |
| `postgres_dw`   | PostgreSQL | Data Warehouse        |
| `minio_s3`      | S3         | MinIO Data Lake       |

## Types de tÃ¢ches

### SparkSubmitOperator

Pour les jobs Spark (EX01, EX02, EX05) :

```python
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

ex01_task = SparkSubmitOperator(
    task_id='ex01_data_retrieval',
    application='/opt/workdir/ex01_data_retrieval/target/scala-2.12/ex01-data-retrieval_2.12-0.1.0.jar',
    conn_id='spark_default',
    java_class='Ex01DataRetrieval',
    application_args=['--year', '{{ ds[:4] }}', '--month', '{{ ds[5:7] }}'],
)
```

### PostgresOperator

Pour les scripts SQL (EX03) :

```python
from airflow.providers.postgres.operators.postgres import PostgresOperator

ex03_load = PostgresOperator(
    task_id='ex03_dw_load',
    postgres_conn_id='postgres_dw',
    sql='dw_load_incremental.sql',
)
```

### BashOperator

Pour les scripts Python (EX05) :

```python
from airflow.operators.bash import BashOperator

ex05_train = BashOperator(
    task_id='ex05_ml_train',
    bash_command='docker exec spark-master spark-submit ... main.py --mode train',
)
```

## Scheduling

| DAG                  | Schedule       | Description              |
|----------------------|----------------|--------------------------|
| nyc_taxi_monthly     | `0 0 1 * *`    | 1er jour de chaque mois  |
| nyc_taxi_daily       | `0 6 * * *`    | Tous les jours Ã  6h      |
| nyc_taxi_manual      | `None`         | DÃ©clenchement manuel     |

## ParamÃ¨tres du DAG

```python
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 1, 1),
}

dag = DAG(
    'nyc_taxi_pipeline',
    default_args=default_args,
    description='NYC Taxi Big Data Pipeline',
    schedule_interval='@monthly',
    catchup=False,
    tags=['nyc-taxi', 'bigdata'],
)
```

## Docker Compose

```yaml
# docker-compose.airflow.yml
version: '3.8'

services:
  airflow-webserver:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://...
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins

  airflow-scheduler:
    image: apache/airflow:2.7.0
    # ...

  airflow-worker:
    image: apache/airflow:2.7.0
    # ...
```

## Monitoring

### Interface Web

- **URL** : http://localhost:8080
- **Credentials** : airflow / airflow (par dÃ©faut)

### FonctionnalitÃ©s

- ğŸ“Š Vue des DAGs et leur statut
- ğŸ“ˆ Graphiques d'exÃ©cution
- ğŸ“‹ Logs des tÃ¢ches
- ğŸ”” Alertes en cas d'Ã©chec
- ğŸ“… Historique des runs

## Commandes utiles

```bash
# DÃ©marrer Airflow
docker compose -f docker-compose.airflow.yml up -d

# Tester un DAG
airflow dags test nyc_taxi_pipeline 2023-01-01

# Lister les DAGs
airflow dags list

# DÃ©clencher un DAG manuellement
airflow dags trigger nyc_taxi_pipeline

# Voir les logs
docker compose -f docker-compose.airflow.yml logs -f airflow-scheduler
```

## DÃ©pendances

```
apache-airflow==2.7.0
apache-airflow-providers-apache-spark
apache-airflow-providers-postgres
apache-airflow-providers-amazon  # Pour S3/MinIO
```

## Prochaines Ã©tapes

1. [ ] CrÃ©er le fichier `docker-compose.airflow.yml`
2. [ ] ImplÃ©menter le DAG principal
3. [ ] Configurer les connections Airflow
4. [ ] Tester chaque tÃ¢che individuellement
5. [ ] Mettre en place les alertes
6. [ ] Documenter les procÃ©dures de recovery

## Statut

â³ **Ã€ implÃ©menter**

---

**Auteur :** MAAOUIA Ahmed â€“ CY Tech Big Data
