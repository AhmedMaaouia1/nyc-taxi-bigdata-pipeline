"""
============================================================================
DAG FULL PIPELINE - Orchestration Compl√®te EX01 ‚Üí EX05
============================================================================
Ce DAG orchestre le pipeline complet mensuel de bout en bout:

  EX01 (Data Retrieval)
    ‚îÇ
    ‚ñº
  EX02 (Data Ingestion)
    ‚îÇ
    ‚îú‚îÄ‚ñ∫ Branch 1: MinIO (interim) ‚îÄ‚îÄ‚îê
    ‚îÇ                               ‚îÇ
    ‚îî‚îÄ‚ñ∫ Branch 2: PostgreSQL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ EX03 (DW Loading)
                                    ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚ñ∫ EX05 (ML Pipeline)

Avantages de ce DAG unique:
  ‚úÖ Vision globale du pipeline
  ‚úÖ Backfill simplifi√© (1 seul DAG √† relancer)
  ‚úÖ D√©pendances explicites et lisibles
  ‚úÖ Id√©al pour pr√©sentation au jury

Idempotence garantie par:
  - EX01: Skip si fichier existe + overwrite MinIO
  - EX02: Overwrite sur partitions MinIO + truncate staging
  - EX03: ON CONFLICT DO NOTHING
  - EX05: Model candidate + promotion conditionnelle

Qualit√© ajout√©e (v2):
  - SLA sur t√¢ches critiques
  - V√©rifications de comptage inter-√©tapes (seuil 80%)
  - Logging structur√©

Auteur: Pipeline NYC Taxi - CY Tech Big Data
============================================================================
"""

from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from airflow.operators.empty import EmptyOperator
from airflow.utils.dates import days_ago
from airflow.exceptions import AirflowException
import urllib.request
import urllib.error
import logging

logger = logging.getLogger(__name__)


# ============================================================================
# CONFIGURATION DU DAG
# ============================================================================

# Seuils de r√©tention des donn√©es (v√©rification comptage inter-√©tapes)
DATA_RETENTION_MIN_THRESHOLD = 0.80  # 80% minimum, sinon FAIL
DATA_RETENTION_WARN_THRESHOLD = 0.90  # 90% minimum pour warning


def sla_miss_callback(dag, task_list, blocking_task_list, slas, blocking_tis):
    """Callback appel√© quand un SLA est manqu√©."""
    logger.error(
        f"üö® SLA MISS ALERT!\n"
        f"DAG: {dag.dag_id}\n"
        f"Tasks: {[t.task_id for t in task_list]}\n"
        f"Blocked by: {[t.task_id for t in blocking_tis] if blocking_tis else 'N/A'}"
    )


default_args = {
    'owner': 'nyc-taxi-pipeline',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='full_nyc_taxi_pipeline',
    default_args=default_args,
    description='Pipeline complet NYC Taxi: EX01 ‚Üí EX02 ‚Üí EX03 ‚Üí EX05 (avec SLA et quality checks)',
    schedule_interval='@monthly',
    start_date=datetime(2023, 1, 1),
    end_date=datetime(2023, 5, 31),  # Janvier √† Mai 2023 (5 mois)
    catchup=True,  # Active le backfill
    max_active_runs=1,
    tags=['full-pipeline', 'ex01', 'ex02', 'ex03', 'ex05', 'production', 'sla-monitored'],
    doc_md=__doc__,
    sla_miss_callback=sla_miss_callback,
)


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def check_data_availability(**context):
    """
    V√©rifie si les donn√©es du mois sont disponibles sur le site TLC.
    Short-circuit si donn√©es non disponibles.
    """
    execution_date = context['execution_date']
    year = execution_date.year
    month = execution_date.month
    
    url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month:02d}.parquet"
    
    logger.info(f"üîç V√©rification disponibilit√©: {url}")
    
    try:
        req = urllib.request.Request(url, method='HEAD')
        with urllib.request.urlopen(req, timeout=30) as response:
            if response.status == 200:
                logger.info(f"‚úÖ Donn√©es disponibles pour {year}-{month:02d}")
                return True
    except urllib.error.HTTPError as e:
        logger.warning(f"‚ö†Ô∏è Donn√©es non disponibles ({e.code}): {url}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        return False
    
    return False


def verify_row_count_retention(
    source_count: int,
    target_count: int,
    source_name: str,
    target_name: str,
    **context
) -> bool:
    """
    V√©rifie le taux de r√©tention entre deux √©tapes.
    
    Seuils:
    - < 80% : FAIL (perte de donn√©es critique)
    - < 90% : WARNING (logs mais continue)
    - >= 90% : OK
    
    Returns:
        True si OK/WARNING, False si FAIL
    """
    if source_count == 0:
        logger.error(f"‚ùå {source_name} has 0 rows - cannot compute retention")
        raise AirflowException(f"Source {source_name} has 0 rows")
    
    retention = target_count / source_count
    retention_pct = retention * 100
    lost_rows = source_count - target_count
    
    logger.info(f"üìä RETENTION CHECK: {source_name} ‚Üí {target_name}")
    logger.info(f"   Source: {source_count:,} rows")
    logger.info(f"   Target: {target_count:,} rows")
    logger.info(f"   Retention: {retention_pct:.1f}%")
    
    if retention < DATA_RETENTION_MIN_THRESHOLD:
        logger.error(
            f"‚ùå RETENTION FAILED: {retention_pct:.1f}% < {DATA_RETENTION_MIN_THRESHOLD*100}% minimum | "
            f"Lost {lost_rows:,} rows ({source_count:,} ‚Üí {target_count:,})"
        )
        raise AirflowException(
            f"Data retention too low: {retention_pct:.1f}% < {DATA_RETENTION_MIN_THRESHOLD*100}%"
        )
    elif retention < DATA_RETENTION_WARN_THRESHOLD:
        logger.warning(
            f"‚ö†Ô∏è RETENTION WARNING: {retention_pct:.1f}% < {DATA_RETENTION_WARN_THRESHOLD*100}% | "
            f"Lost {lost_rows:,} rows"
        )
    else:
        logger.info(f"‚úÖ RETENTION OK: {retention_pct:.1f}%")
    
    # Store in XCom for reporting
    context['ti'].xcom_push(key=f'{target_name}_count', value=target_count)
    context['ti'].xcom_push(key=f'{source_name}_to_{target_name}_retention', value=retention_pct)
    
    return True


def log_pipeline_start(**context):
    """Log le d√©but du pipeline avec tous les param√®tres."""
    execution_date = context['execution_date']
    year = execution_date.year
    month = execution_date.month
    
    # Calcul fen√™tre glissante pour EX05
    train_months = []
    for i in range(3, 0, -1):
        m = execution_date - relativedelta(months=i)
        train_months.append(m.strftime('%Y-%m'))
    
    logger.info("=" * 70)
    logger.info("üöÄ NYC TAXI FULL PIPELINE - D√âMARRAGE")
    logger.info("=" * 70)
    logger.info(f"   üìÖ Execution Date     : {execution_date}")
    logger.info(f"   üìÜ P√©riode trait√©e    : {year}-{month:02d}")
    logger.info(f"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.info(f"   EX01 - Data Retrieval")
    logger.info(f"   Source    : https://d37ci6vzurychx.cloudfront.net/trip-data/")
    logger.info(f"   Fichier   : yellow_tripdata_{year}-{month:02d}.parquet")
    logger.info(f"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.info(f"   EX02 - Data Ingestion")
    logger.info(f"   Branch 1  : s3a://nyc-interim/yellow/{year}/{month:02d}/")
    logger.info(f"   Branch 2  : PostgreSQL staging (yellow_trips_staging)")
    logger.info(f"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.info(f"   EX03 - DW Loading ‚Üí fact_trip + dimensions")
    logger.info(f"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.info(f"   EX05 - ML Pipeline")
    logger.info(f"   Train     : {', '.join(train_months)}")
    logger.info(f"   Test      : {year}-{month:02d}")
    logger.info("=" * 70)


def compute_ml_params(**context):
    """Calcule et push les param√®tres ML vers XCom."""
    execution_date = context['execution_date']
    test_month = execution_date.strftime('%Y-%m')
    
    train_months = []
    for i in range(3, 0, -1):
        m = execution_date - relativedelta(months=i)
        train_months.append(m.strftime('%Y-%m'))
    
    train_months_str = ','.join(train_months)
    
    context['ti'].xcom_push(key='test_month', value=test_month)
    context['ti'].xcom_push(key='train_months', value=train_months_str)
    
    logger.info(f"üìä ML Parameters computed: test={test_month}, train={train_months_str}")


def should_run_ml(**context):
    """
    V√©rifie si on peut ex√©cuter le ML (besoin d'au moins 3 mois de donn√©es).
    EX05 d√©marre √† partir d'avril 2023 (besoin de janv-mars pour training).
    """
    execution_date = context['execution_date']
    # Comparer avec un datetime naive (sans timezone)
    min_date = datetime(2023, 4, 1)
    
    # Convertir execution_date en naive si n√©cessaire
    if hasattr(execution_date, 'replace') and execution_date.tzinfo is not None:
        execution_date_naive = execution_date.replace(tzinfo=None)
    else:
        execution_date_naive = execution_date
    
    if execution_date_naive >= min_date:
        logger.info(f"‚úÖ ML peut s'ex√©cuter: {execution_date} >= {min_date}")
        return True
    else:
        logger.info(f"‚è≠Ô∏è ML skip: {execution_date} < {min_date} (pas assez de donn√©es training)")
        return False


# ============================================================================
# D√âFINITION DES T√ÇCHES
# ============================================================================

with dag:
    
    # ========================================================================
    # PHASE 0: INITIALISATION
    # ========================================================================
    
    start = EmptyOperator(
        task_id='start_pipeline',
    )
    
    log_start = PythonOperator(
        task_id='log_pipeline_params',
        python_callable=log_pipeline_start,
        provide_context=True,
    )
    
    check_source_available = ShortCircuitOperator(
        task_id='check_source_data_available',
        python_callable=check_data_availability,
        provide_context=True,
    )
    
    # ========================================================================
    # PHASE 1: EX01 - DATA RETRIEVAL
    # ========================================================================
    
    ex01_start = EmptyOperator(
        task_id='ex01_start',
    )
    
    ex01_spark_submit = BashOperator(
        task_id='ex01_spark_submit',
        bash_command="""
            echo "üöÄ EX01 - Data Retrieval"
            docker exec spark-master spark-submit \
                --class Ex01DataRetrieval \
                --master spark://spark-master:7077 \
                --deploy-mode client \
                --driver-memory 2g \
                --executor-memory 2g \
                /opt/workdir/ex01_data_retrieval/target/scala-2.12/ex01-data-retrieval_2.12-0.1.0.jar \
                --year {{ execution_date.year }} \
                --month {{ execution_date.strftime('%m') }}
        """,
        execution_timeout=timedelta(hours=1),
        sla=timedelta(minutes=30),  # SLA: 30 minutes max
        retries=3,
        retry_delay=timedelta(minutes=2),
    )
    
    ex01_verify = BashOperator(
        task_id='ex01_verify_output',
        bash_command="""
            echo "üîç V√©rification EX01..."
            # Get raw file count for quality check
            RAW_COUNT=$(docker exec minio-mc mc ls --summarize local/nyc-raw/yellow/{{ execution_date.year }}/{{ execution_date.strftime('%m') }}/ | grep "Total Size:" | head -1)
            echo "Raw data: $RAW_COUNT"
            docker exec minio-mc mc ls local/nyc-raw/yellow/{{ execution_date.year }}/{{ execution_date.strftime('%m') }}/ || exit 1
            echo "‚úÖ EX01 OK"
        """,
    )
    
    ex01_end = EmptyOperator(
        task_id='ex01_complete',
    )
    
    # ========================================================================
    # PHASE 2: EX02 - DATA INGESTION
    # ========================================================================
    
    ex02_start = EmptyOperator(
        task_id='ex02_start',
    )
    
    ex02_spark_submit = BashOperator(
        task_id='ex02_spark_submit',
        bash_command="""
            echo "üöÄ EX02 - Data Ingestion (Double Branche)"
            docker exec \
                -e POSTGRES_HOST=postgres \
                -e POSTGRES_PORT=5432 \
                -e POSTGRES_DB=nyc_dw \
                -e POSTGRES_USER=nyc \
                -e POSTGRES_PASSWORD=nyc123 \
                -e MINIO_ENDPOINT=minio:9000 \
                -e MINIO_ACCESS_KEY=minioadmin \
                -e MINIO_SECRET_KEY=minioadmin \
                spark-master spark-submit \
                --class Ex02DataIngestion \
                --master spark://spark-master:7077 \
                --deploy-mode client \
                --driver-memory 2g \
                --executor-memory 2g \
                --jars /opt/spark/jars/postgresql-42.7.4.jar \
                /opt/workdir/ex02_data_ingestion/target/scala-2.12/ex02-data-ingestion_2.12-0.1.0.jar \
                --year {{ execution_date.year }} \
                --month {{ execution_date.strftime('%m') }} \
                --enableDw true
        """,
        execution_timeout=timedelta(hours=2),
        sla=timedelta(hours=1, minutes=30),  # SLA: 1h30 max
        retries=3,
        retry_delay=timedelta(minutes=2),
    )
    
    ex02_verify_branch1 = BashOperator(
        task_id='ex02_verify_minio_interim',
        bash_command="""
            echo "üîç V√©rification Branch 1 (MinIO interim)..."
            docker exec minio-mc mc ls local/nyc-interim/yellow/{{ execution_date.year }}/{{ execution_date.strftime('%m') }}/ || exit 1
            echo "‚úÖ Branch 1 OK"
        """,
    )
    
    ex02_verify_branch2 = BashOperator(
        task_id='ex02_verify_postgres_staging',
        bash_command="""
            echo "üîç V√©rification Branch 2 (PostgreSQL staging)..."
            STAGING_COUNT=$(docker exec postgres psql -U nyc -d nyc_dw -t -c \
                "SELECT COUNT(*) FROM yellow_trips_staging;")
            echo "Staging count: $STAGING_COUNT"
            # Output count for XCom
            echo "::set-output staging_count=$STAGING_COUNT"
            [ "$STAGING_COUNT" -gt 0 ] || exit 1
            echo "‚úÖ Branch 2 OK"
        """,
    )
    
    # V√©rification de la r√©tention des donn√©es entre raw et staging
    ex02_quality_check = BashOperator(
        task_id='ex02_quality_check_retention',
        bash_command="""
            echo "üìä EX02 Quality Check - V√©rification r√©tention donn√©es..."
            
            # Get staging count
            STAGING_COUNT=$(docker exec postgres psql -U nyc -d nyc_dw -t -c \
                "SELECT COUNT(*) FROM yellow_trips_staging;" | tr -d ' ')
            
            echo "Staging rows: $STAGING_COUNT"
            
            # V√©rification: staging doit avoir des donn√©es
            if [ "$STAGING_COUNT" -lt 1000 ]; then
                echo "‚ùå QUALITY CHECK FAILED: Staging has only $STAGING_COUNT rows (min 1000 expected)"
                exit 1
            fi
            
            echo "‚úÖ Quality check passed: $STAGING_COUNT rows in staging"
        """,
    )
    
    ex02_end = EmptyOperator(
        task_id='ex02_complete',
        trigger_rule='all_success',
    )
    
    # ========================================================================
    # PHASE 3: EX03 - DW LOADING
    # ========================================================================
    
    ex03_start = EmptyOperator(
        task_id='ex03_start',
    )
    
    ex03_load_dimensions = BashOperator(
        task_id='ex03_load_dimensions',
        bash_command="""
            echo "üìä EX03 - Chargement dimensions..."
            docker exec -i postgres psql -U nyc -d nyc_dw << 'EOSQL'
            
            INSERT INTO dim_vendor (vendor_id)
            SELECT DISTINCT vendorid FROM yellow_trips_staging WHERE vendorid IS NOT NULL
            ON CONFLICT DO NOTHING;
            
            INSERT INTO dim_payment_type (payment_type_id)
            SELECT DISTINCT payment_type FROM yellow_trips_staging WHERE payment_type IS NOT NULL
            ON CONFLICT DO NOTHING;
            
            INSERT INTO dim_ratecode (ratecode_id)
            SELECT DISTINCT ratecodeid FROM yellow_trips_staging WHERE ratecodeid IS NOT NULL
            ON CONFLICT DO NOTHING;
            
            INSERT INTO dim_location (location_id)
            SELECT DISTINCT pulocationid FROM yellow_trips_staging WHERE pulocationid IS NOT NULL
            ON CONFLICT DO NOTHING;
            
            INSERT INTO dim_location (location_id)
            SELECT DISTINCT dolocationid FROM yellow_trips_staging WHERE dolocationid IS NOT NULL
            ON CONFLICT DO NOTHING;
            
            INSERT INTO dim_date (date_id, year, month, day, day_of_week)
            SELECT DISTINCT
              DATE(tpep_pickup_datetime),
              EXTRACT(YEAR FROM tpep_pickup_datetime)::INTEGER,
              EXTRACT(MONTH FROM tpep_pickup_datetime)::INTEGER,
              EXTRACT(DAY FROM tpep_pickup_datetime)::INTEGER,
              EXTRACT(DOW FROM tpep_pickup_datetime)::INTEGER
            FROM yellow_trips_staging
            ON CONFLICT DO NOTHING;
            
            INSERT INTO dim_time (time_id, hour, minute)
            SELECT DISTINCT
              CAST(tpep_pickup_datetime AS TIME),
              EXTRACT(HOUR FROM tpep_pickup_datetime)::INTEGER,
              EXTRACT(MINUTE FROM tpep_pickup_datetime)::INTEGER
            FROM yellow_trips_staging
            ON CONFLICT DO NOTHING;
            
EOSQL
            echo "‚úÖ Dimensions charg√©es"
        """,
        execution_timeout=timedelta(hours=1),
    )
    
    ex03_load_facts = BashOperator(
        task_id='ex03_load_fact_trip',
        bash_command="""
            echo "üìä EX03 - Chargement fact_trip..."
            docker exec -i postgres psql -U nyc -d nyc_dw << 'EOSQL'
            
            INSERT INTO fact_trip (
              pickup_date, pickup_time, pickup_location_id, dropoff_location_id,
              vendor_id, payment_type_id, ratecode_id, passenger_count, trip_distance,
              fare_amount, extra, mta_tax, tip_amount, tolls_amount,
              improvement_surcharge, congestion_surcharge, airport_fee, total_amount
            )
            SELECT
              DATE(tpep_pickup_datetime), CAST(tpep_pickup_datetime AS TIME),
              pulocationid, dolocationid, vendorid, payment_type, ratecodeid,
              passenger_count, trip_distance, fare_amount, extra, mta_tax,
              tip_amount, tolls_amount, improvement_surcharge, congestion_surcharge,
              airport_fee, total_amount
            FROM yellow_trips_staging
            ON CONFLICT DO NOTHING;
            
EOSQL
            echo "‚úÖ Fact table charg√©e"
        """,
        execution_timeout=timedelta(hours=1),
        sla=timedelta(hours=1),  # SLA: 1h max
    )
    
    ex03_verify = BashOperator(
        task_id='ex03_verify_dw',
        bash_command="""
            echo "üìä V√©rification DW..."
            docker exec postgres psql -U nyc -d nyc_dw -c \
                "SELECT 'fact_trip' as tbl, COUNT(*) FROM fact_trip UNION ALL SELECT 'dim_date', COUNT(*) FROM dim_date;"
            
            # Quality check: verify data was loaded
            FACT_COUNT=$(docker exec postgres psql -U nyc -d nyc_dw -t -c \
                "SELECT COUNT(*) FROM fact_trip;" | tr -d ' ')
            
            echo "Fact trip count: $FACT_COUNT"
            
            if [ "$FACT_COUNT" -lt 1000 ]; then
                echo "‚ùå DW Quality Check FAILED: fact_trip has only $FACT_COUNT rows"
                exit 1
            fi
            
            echo "‚úÖ EX03 OK - $FACT_COUNT rows in fact_trip"
        """,
    )
    
    ex03_end = EmptyOperator(
        task_id='ex03_complete',
    )
    
    # ========================================================================
    # PHASE 4: EX05 - ML PIPELINE
    # ========================================================================
    
    ex05_check = ShortCircuitOperator(
        task_id='ex05_check_can_run',
        python_callable=should_run_ml,
        provide_context=True,
    )
    
    ex05_compute_params = PythonOperator(
        task_id='ex05_compute_ml_params',
        python_callable=compute_ml_params,
        provide_context=True,
    )
    
    ex05_start = EmptyOperator(
        task_id='ex05_start',
    )
    
    ex05_run_ml = BashOperator(
        task_id='ex05_run_ml_pipeline',
        bash_command="""
            TEST_MONTH="{{ ti.xcom_pull(task_ids='ex05_compute_ml_params', key='test_month') }}"
            TRAIN_MONTHS="{{ ti.xcom_pull(task_ids='ex05_compute_ml_params', key='train_months') }}"
            
            echo "üöÄ EX05 - ML Pipeline"
            echo "   Test: $TEST_MONTH"
            echo "   Train: $TRAIN_MONTHS"
            
            docker exec spark-master spark-submit \
                --master spark://spark-master:7077 \
                --deploy-mode client \
                --driver-memory 1g \
                --executor-memory 1g \
                /opt/workdir/ex05_ml_prediction_service/src/ml_pipeline.py \
                --test-month "$TEST_MONTH" \
                --train-months "$TRAIN_MONTHS" \
                --model-registry-path /opt/workdir/ex05_ml_prediction_service/models/registry \
                --data-base-path s3a://nyc-interim/yellow \
                --reports-dir /opt/workdir/ex05_ml_prediction_service/reports \
                --skip-missing
        """,
        execution_timeout=timedelta(hours=3),
        sla=timedelta(hours=2, minutes=30),  # SLA: 2h30 max
        retries=2,
        retry_delay=timedelta(minutes=5),
    )
    
    ex05_verify = BashOperator(
        task_id='ex05_verify_outputs',
        bash_command="""
            echo "üìä V√©rification outputs ML..."
            docker exec spark-master ls -la /opt/workdir/ex05_ml_prediction_service/reports/
            echo "‚úÖ EX05 OK"
        """,
    )
    
    ex05_end = EmptyOperator(
        task_id='ex05_complete',
    )
    
    # ========================================================================
    # PHASE 5: FIN DU PIPELINE
    # ========================================================================
    
    pipeline_success = EmptyOperator(
        task_id='pipeline_success',
        trigger_rule='none_failed_min_one_success',
    )
    
    log_completion = BashOperator(
        task_id='log_pipeline_completion',
        bash_command="""
            echo "=========================================="
            echo "‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS"
            echo "=========================================="
            echo "   Mois trait√©: {{ execution_date.year }}-{{ execution_date.strftime('%m') }}"
            echo "   Date fin: $(date)"
            echo "=========================================="
        """,
        trigger_rule='none_failed_min_one_success',
    )
    
    # ========================================================================
    # D√âFINITION DES D√âPENDANCES
    # ========================================================================
    
    # Phase 0: Init
    start >> log_start >> check_source_available
    
    # Phase 1: EX01
    check_source_available >> ex01_start >> ex01_spark_submit >> ex01_verify >> ex01_end
    
    # Phase 2: EX02
    ex01_end >> ex02_start >> ex02_spark_submit
    ex02_spark_submit >> [ex02_verify_branch1, ex02_verify_branch2]
    ex02_verify_branch2 >> ex02_quality_check
    [ex02_verify_branch1, ex02_quality_check] >> ex02_end
    
    # Phase 3: EX03 (d√©pend de Branch 2)
    ex02_end >> ex03_start >> ex03_load_dimensions >> ex03_load_facts >> ex03_verify >> ex03_end
    
    # Phase 4: EX05 (d√©pend de Branch 1, peut s'ex√©cuter en parall√®le de EX03)
    ex02_end >> ex05_check >> ex05_compute_params >> ex05_start >> ex05_run_ml >> ex05_verify >> ex05_end
    
    # Phase 5: Fin
    [ex03_end, ex05_end] >> pipeline_success >> log_completion
